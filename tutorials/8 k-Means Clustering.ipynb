{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* In the previous few section, we have explored one category of unsupervised machine learning models: dimensionality reduction.\n",
    "* Here we will move on to another class of unsupervised machine learning models: clustering algorithms\n",
    "* Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.\n",
    "* Many clustering algorithms are available in Scikit-Learn, but the simplest to understand is k-means clustering.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Introducing k-Means\n",
    "* The k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.\n",
    "* It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
    "    * The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "    * Each point is closer to its own cluster center than to other cluster centers.\n",
    "\n",
    "First, let's generate a two-dimensional dataset containing four distinct blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Here, it's relatively easy to pick out the four clusters.\n",
    "* The k-means algorithm does this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's visualize the results by plotting the data colored by these labels. We will also plot the cluster centers as determined by the k-means estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* The k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. \n",
    "* You might wonder how this algorithm finds these clusters so quickly!\n",
    "* After all, the number of possible combinations of cluster assignments is exponential in the number of data points\n",
    "* An exhaustive search would be very, very costly. \n",
    "* The approach to k-means clustering is called _expectation–maximization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Expectation–Maximization\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "    * Assign points to the nearest cluster center\n",
    "    * Set the cluster centers to the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Interactive Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "def plot_kmeans_interactive(min_clusters=1, max_clusters=6):\n",
    "    X, y = make_blobs(n_samples=300, centers=4,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "        \n",
    "    def plot_points(X, labels, n_clusters):\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis',\n",
    "                    vmin=0, vmax=n_clusters - 1);\n",
    "            \n",
    "    def plot_centers(centers):\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=np.arange(centers.shape[0]),\n",
    "                    s=200, cmap='viridis')\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c='black', s=50)\n",
    "        \n",
    "    def _kmeans_step(frame=0, n_clusters=4):\n",
    "        rng = np.random.RandomState(2)\n",
    "        labels = np.zeros(X.shape[0])\n",
    "        centers = rng.randn(n_clusters, 2)\n",
    "\n",
    "        nsteps = frame // 3\n",
    "\n",
    "        for i in range(nsteps + 1):\n",
    "            old_centers = centers\n",
    "            if i < nsteps or frame % 3 > 0:\n",
    "                labels = pairwise_distances_argmin(X, centers)\n",
    "\n",
    "            if i < nsteps or frame % 3 > 1:\n",
    "                centers = np.array([X[labels == j].mean(0)\n",
    "                                    for j in range(n_clusters)])\n",
    "                nans = np.isnan(centers)\n",
    "                centers[nans] = old_centers[nans]\n",
    "\n",
    "        # plot the data and cluster centers\n",
    "        plot_points(X, labels, n_clusters)\n",
    "        plot_centers(old_centers)\n",
    "\n",
    "        # plot new centers if third frame\n",
    "        if frame % 3 == 2:\n",
    "            for i in range(n_clusters):\n",
    "                plt.annotate('', centers[i], old_centers[i], \n",
    "                             arrowprops=dict(arrowstyle='->', linewidth=1))\n",
    "            plot_centers(centers)\n",
    "\n",
    "        plt.xlim(-4, 4)\n",
    "        plt.ylim(-2, 10)\n",
    "\n",
    "        if frame % 3 == 1:\n",
    "            plt.text(3.8, 9.5, \"1. Reassign points to nearest centroid\",\n",
    "                     ha='right', va='top', size=14)\n",
    "        elif frame % 3 == 2:\n",
    "            plt.text(3.8, 9.5, \"2. Update centroids to cluster means\",\n",
    "                     ha='right', va='top', size=14)\n",
    "            \n",
    "    return interact(_kmeans_step, frame=[0, 50],\n",
    "                    n_clusters=[min_clusters, max_clusters])\n",
    "\n",
    "plot_kmeans_interactive();            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The algorithm is simple enough that we can implement it in just a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Issues of the EM algorithm\n",
    "##### The globally optimal result may not be achieved\n",
    "* There is no assurance that it will lead to the global best solution.\n",
    "* For example, if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "centers, labels = find_clusters(X, 4, rseed=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* It is usual to run the algorith several times with different starting guesses.\n",
    "* Scikit-Learn does by default 10 runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### The number of clusters must be selected beforehand\n",
    "* A challenge with k-means is that you must tell it how many clusters you expect.\n",
    "* It cannot learn the number of clusters from the data. \n",
    "* For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = KMeans(6, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### k-means is limited to linear cluster boundaries\n",
    "* The model assumptions of k-means means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
    "* The boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries.\n",
    "* Consider the following data, along with the cluster labels found by the typical k-means approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### k-means can be slow for large numbers of samples\n",
    "* Because each iteration of k-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example 1: k-means on digits\n",
    "* Let's take a look at applying k-means on the same simple digits data that we already saw.\n",
    "* Here we will attempt to use k-means to try to identify similar digits without using the original label information.\n",
    "\n",
    "Recall that the digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(digits.data)\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What do these clusters look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
    "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "We can have a look at the true labels of the datapoints assigned to a cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cluster_no = 6\n",
    "digits.target[(clusters == cluster_no)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Obviously, there is a mismatch between the cluster numbers and the labels.\n",
    "* We can fix that using the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With just a simple k-means algorithm, we discovered the correct grouping for 80% of the input digits! Let's check the confusion matrix for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Just for fun\n",
    "* We can use the t-distributed stochastic neighbor embedding (t-SNE) algorithm to pre-process the data before performing k-means. \n",
    "* t-SNE is a nonlinear embedding algorithm that is particularly adept at preserving points within clusters.\n",
    "\n",
    "Let's see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Project the data: this step will take several seconds\n",
    "tsne = TSNE(n_components=2, init='random', random_state=0)\n",
    "digits_proj = tsne.fit_transform(digits.data)\n",
    "\n",
    "# Compute the clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(digits_proj)\n",
    "\n",
    "# Permute the labels\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(digits.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
