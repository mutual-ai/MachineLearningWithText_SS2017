{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Earlier we explored an example of using a PCA projection as a feature selector for facial recognition with a support vector machine.  \n",
    "Here we will take a look back and explore a bit more of what went into that. Recall that we were using the Labeled Faces dataset made available through Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the lecture we will apply a PCA dimensionality reduction to 150 dimensions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(150)\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a closer look at the shape of the `components` array, you may notice something. What are the factors that determine the shape of the components array? __Pause for a while and think about it carefully.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think you have discovered the correct answer, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load PCA_solution.py\n",
    "The shape of the components array is [# of principal components, # of features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the principal components have to same size as our features, I want you to interpret each component as an image and plot it. That is, I want you to plot the first 24 components similarly to what we did in the [SVM lecture](../tutorials/5%20Support%20Vector%20Machines.ipynb#Dataset). Of course there are no names to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a close look at the faces, think about the interpretation of these plots. Develop a hypothesis that explains why theses images look the way they are and why it is useful for the task at hand. We will discuss this in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the [lecture](../tutorials/7%20Principal%20Component%20Analysis.ipynb#Intuition-of-PCA) we've talked about the amount of variance each component explains. Plot the cumulative sum of the explained variance.  \n",
    "_Hint 1_: The cumulative sum of `[1, 2, 3]` is `[1, 3, 6]`.  \n",
    "_Hint 2_: Numpy's [`cumsum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cumsum.html) may come in handy.  \n",
    "_Hint 3_: Don't use `pca.explained_variance_`. Instead use `pca.explained_variance_ratio_` which makes the variance sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you accomplished the previous task you may have come to the comclusion that these 150 components account for nearly 95% of the variance. So what happens if we reconstruct the images from those 150 components?  \n",
    "Compare the input images (say, the first 10 of our dataset) with the images reconstructed from these 150 components, by ploting them alongside.  \n",
    "_Hint_: You have to fit the model, transform the data and then do the [inverse transform](../tutorials/7%20Principal%20Component%20Analysis.ipynb#Dimensionality-reduction) that we saw already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
